{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction import text\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import plot_roc_curve\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import log_loss\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "\n",
        "def Q1():\n",
        "    twenty_all_train = fetch_20newsgroups()\n",
        "    plt.hist(twenty_all_train.target, bins=20)\n",
        "    plt.title(\"Histogram of documents per topic\")\n",
        "    plt.xlabel(\"Topic\")\n",
        "    plt.ylabel(\"Number of documents\")\n",
        "    plt.show()\n",
        "\n",
        "def penn2morphy(penntag):\n",
        "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
        "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
        "                  'VB':'v', 'RB':'r'}\n",
        "    try:\n",
        "        return morphy_tag[penntag[:2]]\n",
        "    except:\n",
        "        return 'n'\n",
        "\n",
        "def lemmatize_sent(list_word):\n",
        "    # Text input is string, returns array of lowercased strings(words).\n",
        "    wnl = nltk.wordnet.WordNetLemmatizer()\n",
        "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n",
        "            for word, tag in pos_tag(list_word)]\n",
        "\n",
        "def stem_rmv_punc(doc):\n",
        "    analyzer = CountVectorizer().build_analyzer()\n",
        "    stop_words_skt = text.ENGLISH_STOP_WORDS\n",
        "    stop_words_en = stopwords.words('english')\n",
        "    combined_stopwords = set.union(set(stop_words_en),set(punctuation),set(stop_words_skt))\n",
        "    return (word for word in lemmatize_sent(analyzer(doc)) if word not in combined_stopwords and not word.isdigit())\n",
        "\n",
        "def GetLSA(data, k=50):\n",
        "    svd = TruncatedSVD(n_components=k, random_state=0)\n",
        "    data_reduced = svd.fit_transform(data)\n",
        "    error = np.sum(np.array(data - svd.inverse_transform(data_reduced))**2)\n",
        "    return data_reduced, error\n",
        "\n",
        "def GetNMF(data, k=50):\n",
        "    model = NMF(n_components=k, init='random', random_state=0)\n",
        "    data_reduced = model.fit_transform(data)\n",
        "    error = np.sum(np.array(data - data_reduced.dot(model.components_))**2)\n",
        "    return data_reduced, error\n",
        "\n",
        "def GetTFData():\n",
        "    comp_categories = ['comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware']\n",
        "    rec_categories = ['rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey']\n",
        "    all_categories = comp_categories + rec_categories\n",
        "    twenty_train = fetch_20newsgroups(subset='train', categories=all_categories, shuffle=True, random_state=None)\n",
        "    twenty_test = fetch_20newsgroups(subset='test', categories=all_categories, shuffle=True, random_state=None)\n",
        "\n",
        "    count_vect = CountVectorizer(min_df=3, analyzer=stem_rmv_punc, stop_words='english')\n",
        "    tfidf_transformer = TfidfTransformer()\n",
        "\n",
        "    X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
        "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "    print(\"Train tf-idf shape:\", X_train_tfidf.shape)\n",
        "\n",
        "    X_test_counts = count_vect.fit_transform(twenty_test.data)\n",
        "    X_test_tfidf = tfidf_transformer.fit_transform(X_test_counts)\n",
        "    print(\"Test tf-idf shape:\", X_test_tfidf.shape)\n",
        "    print()\n",
        "\n",
        "    comp_set = set(comp_categories)\n",
        "    y_train = [int(all_categories[x] in comp_set) for x in twenty_train.target]\n",
        "    y_test = [int(all_categories[x] in comp_set) for x in twenty_test.target]\n",
        "\n",
        "    return X_train_tfidf, X_test_tfidf, y_train, y_test\n",
        "\n",
        "def GetData(get_nmf_error=False):\n",
        "    X_train, X_test, y_train, y_test = GetTFData()\n",
        "\n",
        "    X_train_LSA, error_train_LSA = GetLSA(X_train)\n",
        "    X_test_LSA, error_test_LSA = GetLSA(X_test)\n",
        "    print(\"X_train LSA error:\", error_train_LSA)\n",
        "    print(\"X_test LSA error:\", error_test_LSA)\n",
        "\n",
        "    if get_nmf_error:\n",
        "        _, error_train_NMF = GetNMF(X_train)\n",
        "        _, error_test_NMF = GetNMF(X_test)\n",
        "        print(\"X_train NMF error:\", error_train_NMF)\n",
        "        print(\"X_test NMF error:\", error_test_NMF)\n",
        "    return np.array(X_train_LSA), np.array(X_test_LSA), np.array(y_train), np.array(y_test)\n",
        "\n",
        "def TrainTestModel(model, X_train, y_train, X_test, y_test, name, show_log_loss=False):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    accuracy = float(cm[0][0] + cm[1][1])/(np.sum(cm))\n",
        "    precision = float(cm[0][0]/(cm[0][0] + cm[0][1]))\n",
        "    recall = float(cm[0][0]/(cm[0][0] + cm[1][0]))\n",
        "    F1 = 2/((1/precision) + (1/recall))\n",
        "\n",
        "    print(\"Results for {} -\".format(name))\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1-Score:\", F1)\n",
        "    if show_log_loss:\n",
        "        print(\"Log Loss:\", log_loss(y_test, y_pred))\n",
        "    print()\n",
        "\n",
        "    plot_roc_curve(model, X_test, y_test)\n",
        "    plt.title(name + \" ROC Curve\")\n",
        "    plt.show()\n",
        "\n",
        "def CrossValSVM(X_train, y_train, X_test, y_test):\n",
        "    kf = KFold(n_splits=5)\n",
        "    max_acc = 0.0\n",
        "    max_k = -3\n",
        "    for k in range(-3, 4):\n",
        "        svm = LinearSVC(C=10**k, max_iter=5000)\n",
        "        acc = 0.0\n",
        "        for train_index, test_index in kf.split(X_train):\n",
        "            svm.fit(X_train[train_index], y_train[train_index])\n",
        "            acc += svm.score(X_train[test_index], y_train[test_index])\n",
        "        acc /= 5\n",
        "        if acc > max_acc:\n",
        "            max_acc = acc\n",
        "            max_k = k\n",
        "\n",
        "    print(\"Max cross val accuracy:\", max_acc)\n",
        "    print(\"Max K:\", max_k)\n",
        "    print()\n",
        "    svm = LinearSVC(C=10**max_k, max_iter=5000)\n",
        "    TrainTestModel(svm, X_train, y_train, X_test, y_test, \"Best SVM\")\n",
        "\n",
        "def CrossValLogisticRegression(X_train, y_train, X_test, y_test):\n",
        "    kf = KFold(n_splits=5)\n",
        "    penalties = ['l2', 'l1']\n",
        "    for penalty in penalties:\n",
        "        max_acc = 0.0\n",
        "        max_k = -3\n",
        "        for k in range(-3, 4):\n",
        "            model = LogisticRegression(penalty=penalty, C=10**k, solver='liblinear')\n",
        "            acc = 0.0\n",
        "            for train_index, test_index in kf.split(X_train):\n",
        "                model.fit(X_train[train_index], y_train[train_index])\n",
        "                acc += model.score(X_train[test_index], y_train[test_index])\n",
        "            acc /= 5\n",
        "            if acc > max_acc:\n",
        "                max_acc = acc\n",
        "                max_k = k\n",
        "        \n",
        "        print(\"Max cross val accuracy for {} regulaizer:\".format(penalty), max_acc)\n",
        "        print(\"Max k:\", max_k)\n",
        "        print(\"Coefficients:\", model.coef_)\n",
        "        print()\n",
        "        model = LogisticRegression(penalty=penalty, C=10**max_k, solver='liblinear')\n",
        "        TrainTestModel(model, X_train, y_train, X_test, y_test, penalty + \" Regularized Logistic Regression\", show_log_loss=True)\n",
        "\n",
        "Q1()\n",
        "\n",
        "X_train, X_test, y_train, y_test = GetData()\n",
        "hard_svm = LinearSVC(C=1000.0, max_iter=5000)\n",
        "TrainTestModel(hard_svm, X_train, y_train, X_test, y_test, \"Hard SVM\")\n",
        "\n",
        "soft_svm = LinearSVC(C=0.0001)\n",
        "TrainTestModel(soft_svm, X_train, y_train, X_test, y_test, \"Soft SVM\")\n",
        "\n",
        "CrossValSVM(X_train, y_train, X_test, y_test)\n",
        "\n",
        "no_reg_model = LogisticRegression(penalty='none')\n",
        "TrainTestModel(no_reg_model, X_train, y_train, X_test, y_test, \"No Regularization Logistic Regression\")\n",
        "\n",
        "CrossValLogisticRegression(X_train, y_train, X_test, y_test)\n",
        "\n",
        "NB = GaussianNB()\n",
        "TrainTestModel(NB, X_train, y_train, X_test, y_test, \"GaussianNB\")"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}